{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da99dc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f89d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망구축에 필요한 tensorflow\n",
    "#라이브러리와 numpy 그리고 시각화도구\n",
    "#matplotlib를 import 한다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Variable의 랜덤함수 시드를 설정한다.\n",
    "tf.random.set_seed(777) #변경한 버전에서는 랜덤시드 사용 불가능해서 주석처리함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea62af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99852cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMaxScaler 정의 -> data를 0부터 1사이의 값으로 변환(normalize)\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26d27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train Parameter 상수값 설정\n",
    "seq_length = 7 #7일 단위로 학습시키고 8일째를 예측\n",
    "input_dim = 14 #input 데이터의 개수\n",
    "hidden_dim = 16 #은닉층의 개수\n",
    "output_dim = 1 # target label의 개수(사과가격)\n",
    "learning_rate = 0.01\n",
    "iterations = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afdbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#값 설정이 끝나면 위에서 만든 데이터 셋을 불러온다. 그 후 데이터양에 따라 적절하게 Training Data와 Test Data로 분할한다.\n",
    "#data load\n",
    "#만들어둔 데이터셋을 Load한다.\n",
    "xy = np.loadtxt('./apple_data.csv', delimiter=',')\n",
    "xy = MinMaxScaler(xy) #Normalize\n",
    "x = xy #전체 데이터(input)\n",
    "y = xy[:,[-1]] #target Label(사과가격)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a67968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset\n",
    "#Normalize가 끝난 데이터를 7일단위로 슬라이스 해서 3차원형태로 만든다.\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i: i+seq_length]\n",
    "    _y = y[i+seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#train/test set 나누기\n",
    "train_size = int(len(dataY) * 0.7) #train size = 70%\n",
    "test_size = len(dataY) - train_size #test size = 30%\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd995c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21750663, 0.27918782, 0.        , ..., 0.22323462,\n",
       "         0.11764706, 0.22404372],\n",
       "        [0.25994695, 0.33756345, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.27595628],\n",
       "        [0.29442971, 0.30203046, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.27322404],\n",
       "        ...,\n",
       "        [0.22811671, 0.21827411, 0.        , ..., 0.24829157,\n",
       "         0.11764706, 0.22404372],\n",
       "        [0.28912467, 0.23857868, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.26502732],\n",
       "        [0.19893899, 0.23096447, 0.        , ..., 0.20728929,\n",
       "         0.11764706, 0.22131147]],\n",
       "\n",
       "       [[0.25994695, 0.33756345, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.27595628],\n",
       "        [0.29442971, 0.30203046, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.27322404],\n",
       "        [0.22015915, 0.30964467, 0.        , ..., 0.20045558,\n",
       "         0.11764706, 0.23770492],\n",
       "        ...,\n",
       "        [0.28912467, 0.23857868, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.26502732],\n",
       "        [0.19893899, 0.23096447, 0.        , ..., 0.20728929,\n",
       "         0.11764706, 0.22131147],\n",
       "        [0.3872679 , 0.35532995, 0.        , ..., 0.40318907,\n",
       "         0.11764706, 0.33333333]],\n",
       "\n",
       "       [[0.29442971, 0.30203046, 0.        , ..., 0.25056948,\n",
       "         0.11764706, 0.27322404],\n",
       "        [0.22015915, 0.30964467, 0.        , ..., 0.20045558,\n",
       "         0.11764706, 0.23770492],\n",
       "        [0.22811671, 0.21827411, 0.        , ..., 0.24829157,\n",
       "         0.11764706, 0.22404372],\n",
       "        ...,\n",
       "        [0.19893899, 0.23096447, 0.        , ..., 0.20728929,\n",
       "         0.11764706, 0.22131147],\n",
       "        [0.3872679 , 0.35532995, 0.        , ..., 0.40318907,\n",
       "         0.11764706, 0.33333333],\n",
       "        [0.32891247, 0.26395939, 0.        , ..., 0.30979499,\n",
       "         0.11764706, 0.30054645]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.75862069, 0.65482233, 0.00769231, ..., 0.75626424,\n",
       "         0.80392157, 0.71584699],\n",
       "        [0.75331565, 0.66751269, 0.09230769, ..., 0.82687927,\n",
       "         0.70588235, 0.71311475],\n",
       "        [0.75331565, 0.74365482, 0.00769231, ..., 0.79726651,\n",
       "         0.60784314, 0.74590164],\n",
       "        ...,\n",
       "        [0.74535809, 0.79695431, 0.        , ..., 0.73120729,\n",
       "         0.46078431, 0.77595628],\n",
       "        [0.74801061, 0.76395939, 0.        , ..., 0.74715262,\n",
       "         0.46078431, 0.76229508],\n",
       "        [0.78514589, 0.60152284, 0.54615385, ..., 0.8405467 ,\n",
       "         0.41176471, 0.70218579]],\n",
       "\n",
       "       [[0.75331565, 0.66751269, 0.09230769, ..., 0.82687927,\n",
       "         0.70588235, 0.71311475],\n",
       "        [0.75331565, 0.74365482, 0.00769231, ..., 0.79726651,\n",
       "         0.60784314, 0.74590164],\n",
       "        [0.74270557, 0.78172589, 0.        , ..., 0.74487471,\n",
       "         0.50980392, 0.77595628],\n",
       "        ...,\n",
       "        [0.74801061, 0.76395939, 0.        , ..., 0.74715262,\n",
       "         0.46078431, 0.76229508],\n",
       "        [0.78514589, 0.60152284, 0.54615385, ..., 0.8405467 ,\n",
       "         0.41176471, 0.70218579],\n",
       "        [0.78779841, 0.78680203, 0.        , ..., 0.79726651,\n",
       "         0.31372549, 0.79781421]],\n",
       "\n",
       "       [[0.75331565, 0.74365482, 0.00769231, ..., 0.79726651,\n",
       "         0.60784314, 0.74590164],\n",
       "        [0.74270557, 0.78172589, 0.        , ..., 0.74487471,\n",
       "         0.50980392, 0.77595628],\n",
       "        [0.74535809, 0.79695431, 0.        , ..., 0.73120729,\n",
       "         0.46078431, 0.77595628],\n",
       "        ...,\n",
       "        [0.78514589, 0.60152284, 0.54615385, ..., 0.8405467 ,\n",
       "         0.41176471, 0.70218579],\n",
       "        [0.78779841, 0.78680203, 0.        , ..., 0.79726651,\n",
       "         0.31372549, 0.79781421],\n",
       "        [0.71087533, 0.74111675, 0.        , ..., 0.66514806,\n",
       "         0.31372549, 0.75136612]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9b3f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 7, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24e8030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 7, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00c440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1,n1,r1 = trainX.shape\n",
    "out_trainX = np.column_stack((np.repeat(np.arange(m1),n1),trainX.reshape(m1*n1,-1)))\n",
    "out_df_trainX = pd.DataFrame(out_trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ffefaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217507</td>\n",
       "      <td>0.279188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.427660</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.451852</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134571</td>\n",
       "      <td>0.223235</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.224044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259947</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.699735</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.164733</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.275956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294430</td>\n",
       "      <td>0.302030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316901</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.387566</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160093</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.273224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220159</td>\n",
       "      <td>0.309645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.406383</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.136891</td>\n",
       "      <td>0.200456</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.237705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228117</td>\n",
       "      <td>0.218274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.461702</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.141531</td>\n",
       "      <td>0.248292</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.224044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289125</td>\n",
       "      <td>0.238579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.679894</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.183295</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.265027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198939</td>\n",
       "      <td>0.230964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438298</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.732804</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.373494</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>0.207289</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.221311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.259947</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.699735</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.164733</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.275956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.294430</td>\n",
       "      <td>0.302030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316901</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.387566</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160093</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.273224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220159</td>\n",
       "      <td>0.309645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.406383</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.136891</td>\n",
       "      <td>0.200456</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.237705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2    3         4         5         6         7   \\\n",
       "0  0.0  0.217507  0.279188  0.0  0.183099  0.148148  0.102041  0.427660   \n",
       "1  0.0  0.259947  0.337563  0.0  0.253521  0.283951  0.183673  0.468085   \n",
       "2  0.0  0.294430  0.302030  0.0  0.316901  0.370370  0.306122  0.351064   \n",
       "3  0.0  0.220159  0.309645  0.0  0.183099  0.160494  0.122449  0.406383   \n",
       "4  0.0  0.228117  0.218274  0.0  0.366197  0.382716  0.306122  0.461702   \n",
       "5  0.0  0.289125  0.238579  0.0  0.176056  0.209877  0.163265  0.457447   \n",
       "6  0.0  0.198939  0.230964  0.0  0.042254  0.074074  0.000000  0.438298   \n",
       "7  1.0  0.259947  0.337563  0.0  0.253521  0.283951  0.183673  0.468085   \n",
       "8  1.0  0.294430  0.302030  0.0  0.316901  0.370370  0.306122  0.351064   \n",
       "9  1.0  0.220159  0.309645  0.0  0.183099  0.160494  0.122449  0.406383   \n",
       "\n",
       "         8         9         10        11    12        13        14        15  \\\n",
       "0  0.406977  0.698413  0.020408  0.451852  0.34  0.000000  0.134571  0.223235   \n",
       "1  0.453488  0.699735  0.020408  0.577778  0.24  0.289157  0.164733  0.250569   \n",
       "2  0.186047  0.387566  0.020408  0.651852  0.00  0.000000  0.160093  0.250569   \n",
       "3  0.325581  0.601852  0.020408  0.444444  0.21  0.120482  0.136891  0.200456   \n",
       "4  0.720930  0.787037  0.020408  0.000000  0.61  0.614458  0.141531  0.248292   \n",
       "5  0.500000  0.679894  0.020408  0.022222  0.51  0.590361  0.183295  0.250569   \n",
       "6  0.534884  0.732804  0.040816  0.222222  0.61  0.373494  0.143852  0.207289   \n",
       "7  0.453488  0.699735  0.020408  0.577778  0.24  0.289157  0.164733  0.250569   \n",
       "8  0.186047  0.387566  0.020408  0.651852  0.00  0.000000  0.160093  0.250569   \n",
       "9  0.325581  0.601852  0.020408  0.444444  0.21  0.120482  0.136891  0.200456   \n",
       "\n",
       "         16        17  \n",
       "0  0.117647  0.224044  \n",
       "1  0.117647  0.275956  \n",
       "2  0.117647  0.273224  \n",
       "3  0.117647  0.237705  \n",
       "4  0.117647  0.224044  \n",
       "5  0.117647  0.265027  \n",
       "6  0.117647  0.221311  \n",
       "7  0.117647  0.275956  \n",
       "8  0.117647  0.273224  \n",
       "9  0.117647  0.237705  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df_trainX.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c630af",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2,n2,r2 = testX.shape\n",
    "out_testX = np.column_stack((np.repeat(np.arange(m2),n2),testX.reshape(m2*n2,-1)))\n",
    "out_df_testX = pd.DataFrame(out_testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec2238c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742706</td>\n",
       "      <td>0.781726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.825532</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.662698</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.703016</td>\n",
       "      <td>0.744875</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.775956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.745358</td>\n",
       "      <td>0.796954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190141</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.804255</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.628307</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.716937</td>\n",
       "      <td>0.731207</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.775956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748011</td>\n",
       "      <td>0.763959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.712297</td>\n",
       "      <td>0.747153</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.762295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785146</td>\n",
       "      <td>0.601523</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.958995</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.598608</td>\n",
       "      <td>0.840547</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.702186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787798</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.308642</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.560847</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.749420</td>\n",
       "      <td>0.797267</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.797814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.167513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.202128</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.255291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.088167</td>\n",
       "      <td>0.246014</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.161202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.114058</td>\n",
       "      <td>0.063452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584507</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.058201</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.223235</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.068306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.144670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373239</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.185106</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>0.220957</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.144809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.286472</td>\n",
       "      <td>0.256345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.503968</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.451852</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.150812</td>\n",
       "      <td>0.312073</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.267760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.236041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.417021</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.656085</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.205011</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.224044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6   \\\n",
       "0      0.0  0.742706  0.781726  0.000000  0.225352  0.185185  0.142857   \n",
       "1      0.0  0.745358  0.796954  0.000000  0.190141  0.111111  0.102041   \n",
       "2      0.0  0.748011  0.763959  0.000000  0.394366  0.358025  0.306122   \n",
       "3      0.0  0.785146  0.601523  0.546154  0.246479  0.222222  0.285714   \n",
       "4      0.0  0.787798  0.786802  0.000000  0.309859  0.308642  0.285714   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "751  107.0  0.185676  0.167513  0.000000  0.880282  0.765432  0.714286   \n",
       "752  107.0  0.114058  0.063452  0.000000  0.584507  0.629630  0.816327   \n",
       "753  107.0  0.175066  0.144670  0.000000  0.373239  0.358025  0.469388   \n",
       "754  107.0  0.286472  0.256345  0.000000  0.485915  0.469136  0.346939   \n",
       "755  107.0  0.206897  0.236041  0.000000  0.133803  0.209877  0.102041   \n",
       "\n",
       "           7         8         9         10        11    12        13  \\\n",
       "0    0.825532  0.418605  0.662698  0.612245  0.622222  0.16  0.108434   \n",
       "1    0.804255  0.325581  0.628307  0.612245  0.592593  0.39  0.060241   \n",
       "2    0.840426  0.430233  0.724868  0.591837  0.274074  0.78  0.674699   \n",
       "3    0.876596  0.848837  0.958995  0.591837  0.000000  0.93  0.650602   \n",
       "4    0.800000  0.279070  0.560847  0.571429  0.837037  0.24  0.289157   \n",
       "..        ...       ...       ...       ...       ...   ...       ...   \n",
       "751  0.202128  0.116279  0.255291  0.000000  0.511111  0.36  0.433735   \n",
       "752  0.017021  0.069767  0.058201  0.020408  0.674074  0.00  0.000000   \n",
       "753  0.185106  0.116279  0.226190  0.000000  0.370370  0.44  0.120482   \n",
       "754  0.404255  0.441860  0.503968  0.020408  0.451852  0.60  0.722892   \n",
       "755  0.417021  0.488372  0.656085  0.020408  0.222222  0.75  0.578313   \n",
       "\n",
       "           14        15        16        17  \n",
       "0    0.703016  0.744875  0.509804  0.775956  \n",
       "1    0.716937  0.731207  0.460784  0.775956  \n",
       "2    0.712297  0.747153  0.460784  0.762295  \n",
       "3    0.598608  0.840547  0.411765  0.702186  \n",
       "4    0.749420  0.797267  0.313725  0.797814  \n",
       "..        ...       ...       ...       ...  \n",
       "751  0.088167  0.246014  0.215686  0.161202  \n",
       "752  0.032483  0.223235  0.313725  0.068306  \n",
       "753  0.064965  0.220957  0.313725  0.144809  \n",
       "754  0.150812  0.312073  0.352941  0.267760  \n",
       "755  0.155452  0.205011  0.352941  0.224044  \n",
       "\n",
       "[756 rows x 18 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df_testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f49a3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Eager execution of tf.constant with unsupported shape. Tensor [[[0.21750663 0.27918783 0.         ... 0.22323462 0.11764706 0.22404371]\n  [0.25994694 0.33756346 0.         ... 0.25056946 0.11764706 0.27595627]\n  [0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  ...\n  [0.2281167  0.21827412 0.         ... 0.24829157 0.11764706 0.22404371]\n  [0.28912467 0.23857868 0.         ... 0.25056946 0.11764706 0.2650273 ]\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]]\n\n [[0.25994694 0.33756346 0.         ... 0.25056946 0.11764706 0.27595627]\n  [0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  [0.22015914 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  ...\n  [0.28912467 0.23857868 0.         ... 0.25056946 0.11764706 0.2650273 ]\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]\n  [0.38726792 0.35532996 0.         ... 0.40318906 0.11764706 0.33333334]]\n\n [[0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  [0.22015914 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  [0.2281167  0.21827412 0.         ... 0.24829157 0.11764706 0.22404371]\n  ...\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]\n  [0.38726792 0.35532996 0.         ... 0.40318906 0.11764706 0.33333334]\n  [0.32891247 0.26395938 0.         ... 0.309795   0.11764706 0.30054644]]\n\n ...\n\n [[0.7586207  0.65482235 0.00769231 ... 0.7562642  0.8039216  0.715847  ]\n  [0.7533156  0.6675127  0.09230769 ... 0.82687926 0.7058824  0.71311474]\n  [0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  ...\n  [0.7453581  0.79695433 0.         ... 0.7312073  0.46078432 0.7759563 ]\n  [0.74801064 0.7639594  0.         ... 0.7471526  0.46078432 0.76229507]\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]]\n\n [[0.7533156  0.6675127  0.09230769 ... 0.82687926 0.7058824  0.71311474]\n  [0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  [0.7427056  0.7817259  0.         ... 0.7448747  0.50980395 0.7759563 ]\n  ...\n  [0.74801064 0.7639594  0.         ... 0.7471526  0.46078432 0.76229507]\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]\n  [0.7877984  0.78680205 0.         ... 0.7972665  0.3137255  0.7978142 ]]\n\n [[0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  [0.7427056  0.7817259  0.         ... 0.7448747  0.50980395 0.7759563 ]\n  [0.7453581  0.79695433 0.         ... 0.7312073  0.46078432 0.7759563 ]\n  ...\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]\n  [0.7877984  0.78680205 0.         ... 0.7972665  0.3137255  0.7978142 ]\n  [0.71087533 0.74111676 0.         ... 0.6651481  0.3137255  0.75136614]]] (converted from [[[0.21750663 0.27918782 0.         ... 0.22323462 0.11764706 0.22404372]\n  [0.25994695 0.33756345 0.         ... 0.25056948 0.11764706 0.27595628]\n  [0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  ...\n  [0.22811671 0.21827411 0.         ... 0.24829157 0.11764706 0.22404372]\n  [0.28912467 0.23857868 0.         ... 0.25056948 0.11764706 0.26502732]\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]]\n\n [[0.25994695 0.33756345 0.         ... 0.25056948 0.11764706 0.27595628]\n  [0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  [0.22015915 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  ...\n  [0.28912467 0.23857868 0.         ... 0.25056948 0.11764706 0.26502732]\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]\n  [0.3872679  0.35532995 0.         ... 0.40318907 0.11764706 0.33333333]]\n\n [[0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  [0.22015915 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  [0.22811671 0.21827411 0.         ... 0.24829157 0.11764706 0.22404372]\n  ...\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]\n  [0.3872679  0.35532995 0.         ... 0.40318907 0.11764706 0.33333333]\n  [0.32891247 0.26395939 0.         ... 0.30979499 0.11764706 0.30054645]]\n\n ...\n\n [[0.75862069 0.65482233 0.00769231 ... 0.75626424 0.80392157 0.71584699]\n  [0.75331565 0.66751269 0.09230769 ... 0.82687927 0.70588235 0.71311475]\n  [0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  ...\n  [0.74535809 0.79695431 0.         ... 0.73120729 0.46078431 0.77595628]\n  [0.74801061 0.76395939 0.         ... 0.74715262 0.46078431 0.76229508]\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]]\n\n [[0.75331565 0.66751269 0.09230769 ... 0.82687927 0.70588235 0.71311475]\n  [0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  [0.74270557 0.78172589 0.         ... 0.74487471 0.50980392 0.77595628]\n  ...\n  [0.74801061 0.76395939 0.         ... 0.74715262 0.46078431 0.76229508]\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]\n  [0.78779841 0.78680203 0.         ... 0.79726651 0.31372549 0.79781421]]\n\n [[0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  [0.74270557 0.78172589 0.         ... 0.74487471 0.50980392 0.77595628]\n  [0.74535809 0.79695431 0.         ... 0.73120729 0.46078431 0.77595628]\n  ...\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]\n  [0.78779841 0.78680203 0.         ... 0.79726651 0.31372549 0.79781421]\n  [0.71087533 0.74111675 0.         ... 0.66514806 0.31372549 0.75136612]]]) has 29631 elements, but got `shape` (3,) with 3 elements).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5948/3377966559.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#X = tf.placeholder(tf.float32, [None, seq_length, input_dim]) #3차원 형태의 input placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#X = tf.Tensor([None seq_length input_dim], shape=(3,), dtype=float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Y = tf.placeholder(tf.float32, [None, 1]) #2차원 형태의 output placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m--> 267\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    268\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_eager_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m   raise TypeError(\"Eager execution of tf.constant with unsupported shape. \"\n\u001b[0m\u001b[0;32m    327\u001b[0m                   \u001b[1;34mf\"Tensor {t} (converted from {value}) has {num_t:d} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                   \u001b[1;34mf\"elements, but got `shape` {shape} with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Eager execution of tf.constant with unsupported shape. Tensor [[[0.21750663 0.27918783 0.         ... 0.22323462 0.11764706 0.22404371]\n  [0.25994694 0.33756346 0.         ... 0.25056946 0.11764706 0.27595627]\n  [0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  ...\n  [0.2281167  0.21827412 0.         ... 0.24829157 0.11764706 0.22404371]\n  [0.28912467 0.23857868 0.         ... 0.25056946 0.11764706 0.2650273 ]\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]]\n\n [[0.25994694 0.33756346 0.         ... 0.25056946 0.11764706 0.27595627]\n  [0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  [0.22015914 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  ...\n  [0.28912467 0.23857868 0.         ... 0.25056946 0.11764706 0.2650273 ]\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]\n  [0.38726792 0.35532996 0.         ... 0.40318906 0.11764706 0.33333334]]\n\n [[0.29442972 0.30203044 0.         ... 0.25056946 0.11764706 0.27322406]\n  [0.22015914 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  [0.2281167  0.21827412 0.         ... 0.24829157 0.11764706 0.22404371]\n  ...\n  [0.198939   0.23096447 0.         ... 0.2072893  0.11764706 0.22131148]\n  [0.38726792 0.35532996 0.         ... 0.40318906 0.11764706 0.33333334]\n  [0.32891247 0.26395938 0.         ... 0.309795   0.11764706 0.30054644]]\n\n ...\n\n [[0.7586207  0.65482235 0.00769231 ... 0.7562642  0.8039216  0.715847  ]\n  [0.7533156  0.6675127  0.09230769 ... 0.82687926 0.7058824  0.71311474]\n  [0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  ...\n  [0.7453581  0.79695433 0.         ... 0.7312073  0.46078432 0.7759563 ]\n  [0.74801064 0.7639594  0.         ... 0.7471526  0.46078432 0.76229507]\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]]\n\n [[0.7533156  0.6675127  0.09230769 ... 0.82687926 0.7058824  0.71311474]\n  [0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  [0.7427056  0.7817259  0.         ... 0.7448747  0.50980395 0.7759563 ]\n  ...\n  [0.74801064 0.7639594  0.         ... 0.7471526  0.46078432 0.76229507]\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]\n  [0.7877984  0.78680205 0.         ... 0.7972665  0.3137255  0.7978142 ]]\n\n [[0.7533156  0.74365485 0.00769231 ... 0.7972665  0.60784316 0.74590164]\n  [0.7427056  0.7817259  0.         ... 0.7448747  0.50980395 0.7759563 ]\n  [0.7453581  0.79695433 0.         ... 0.7312073  0.46078432 0.7759563 ]\n  ...\n  [0.7851459  0.60152286 0.54615384 ... 0.84054667 0.4117647  0.7021858 ]\n  [0.7877984  0.78680205 0.         ... 0.7972665  0.3137255  0.7978142 ]\n  [0.71087533 0.74111676 0.         ... 0.6651481  0.3137255  0.75136614]]] (converted from [[[0.21750663 0.27918782 0.         ... 0.22323462 0.11764706 0.22404372]\n  [0.25994695 0.33756345 0.         ... 0.25056948 0.11764706 0.27595628]\n  [0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  ...\n  [0.22811671 0.21827411 0.         ... 0.24829157 0.11764706 0.22404372]\n  [0.28912467 0.23857868 0.         ... 0.25056948 0.11764706 0.26502732]\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]]\n\n [[0.25994695 0.33756345 0.         ... 0.25056948 0.11764706 0.27595628]\n  [0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  [0.22015915 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  ...\n  [0.28912467 0.23857868 0.         ... 0.25056948 0.11764706 0.26502732]\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]\n  [0.3872679  0.35532995 0.         ... 0.40318907 0.11764706 0.33333333]]\n\n [[0.29442971 0.30203046 0.         ... 0.25056948 0.11764706 0.27322404]\n  [0.22015915 0.30964467 0.         ... 0.20045558 0.11764706 0.23770492]\n  [0.22811671 0.21827411 0.         ... 0.24829157 0.11764706 0.22404372]\n  ...\n  [0.19893899 0.23096447 0.         ... 0.20728929 0.11764706 0.22131147]\n  [0.3872679  0.35532995 0.         ... 0.40318907 0.11764706 0.33333333]\n  [0.32891247 0.26395939 0.         ... 0.30979499 0.11764706 0.30054645]]\n\n ...\n\n [[0.75862069 0.65482233 0.00769231 ... 0.75626424 0.80392157 0.71584699]\n  [0.75331565 0.66751269 0.09230769 ... 0.82687927 0.70588235 0.71311475]\n  [0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  ...\n  [0.74535809 0.79695431 0.         ... 0.73120729 0.46078431 0.77595628]\n  [0.74801061 0.76395939 0.         ... 0.74715262 0.46078431 0.76229508]\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]]\n\n [[0.75331565 0.66751269 0.09230769 ... 0.82687927 0.70588235 0.71311475]\n  [0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  [0.74270557 0.78172589 0.         ... 0.74487471 0.50980392 0.77595628]\n  ...\n  [0.74801061 0.76395939 0.         ... 0.74715262 0.46078431 0.76229508]\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]\n  [0.78779841 0.78680203 0.         ... 0.79726651 0.31372549 0.79781421]]\n\n [[0.75331565 0.74365482 0.00769231 ... 0.79726651 0.60784314 0.74590164]\n  [0.74270557 0.78172589 0.         ... 0.74487471 0.50980392 0.77595628]\n  [0.74535809 0.79695431 0.         ... 0.73120729 0.46078431 0.77595628]\n  ...\n  [0.78514589 0.60152284 0.54615385 ... 0.8405467  0.41176471 0.70218579]\n  [0.78779841 0.78680203 0.         ... 0.79726651 0.31372549 0.79781421]\n  [0.71087533 0.74111675 0.         ... 0.66514806 0.31372549 0.75136612]]]) has 29631 elements, but got `shape` (3,) with 3 elements)."
     ]
    }
   ],
   "source": [
    "# 데이터 셋 분할이 끝났으면 해당 데이터를 받을 Placeholder와 인공신경망 구조인 LSTM을 구축한다.\n",
    "#input placeholders\n",
    "#input placeholder의 파라미터로 seq_length,input_dim를 넘겨준다.\n",
    "#X = tf.placeholder(tf.float32, [None, seq_length, input_dim]) #3차원 형태의 input placeholder\n",
    "#X = tf.Tensor([None seq_length input_dim], shape=(3,), dtype=float32)\n",
    "X = tf.constant(trainX, dtype=float, shape=(3,))\n",
    "Y = tf.constant(trainY, dtype=float, shape=(3,))\n",
    "#Y = tf.placeholder(tf.float32, [None, 1]) #2차원 형태의 output placeholder\n",
    "#Y = tf.Tensor([None 1], shape=(2,), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b481c044",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5948/3994463366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#cell = tf.keras.layers.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m cell = tf.keras.layers.LSTMCell(\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'units' is not defined"
     ]
    }
   ],
   "source": [
    "#build a LSTM network (build Rnn)\n",
    "#tensorflow를 이용하면 LSTM 구조를 cell 형태로 빠르고 쉽게 생성해준다.\n",
    "#cell = tf.keras.layers.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "cell = tf.keras.layers.LSTMCell(\n",
    "    units,\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    **kwargs\n",
    ")\n",
    "#생성된 cell을 dynamic_rnn함수로 활성화 시켜준다. 파라미터로 생성된 cell과 input placeholder를 넘겨주고 리턴값으로 output을 받는다.\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n",
    "#그 후 출력된 output을 fully_connected에 통과 시켜 최종 예측값을 리턴 받는다.\n",
    "#Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=tf.tanh)\n",
    "#Y_pred = tf.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=tf.tanh)\n",
    "Y_pred = tf.keras.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=tf.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6b5ee37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5948/2135828751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print (type(Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
